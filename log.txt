INFO 01-25 21:13:51 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.
INFO 01-25 21:13:51 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 01-25 21:13:54 [argparse_utils.py:195] With `vllm serve`, you should provide the model as a positional argument or in a config file instead of via the `--model` option. The `--model` option will be removed in v0.13.
[0;36m(APIServer pid=2159184)[0;0m INFO 01-25 21:13:54 [api_server.py:1278] vLLM API server version 0.1.dev22+gddd454f3a.d20260125
[0;36m(APIServer pid=2159184)[0;0m INFO 01-25 21:13:54 [utils.py:253] non-default args: {'model_tag': '/sharenvme/usershome/cyl/test/model/mistralai/Mixtral-8x7B-Instruct-v0.1', 'trust_request_chat_template': True, 'model': '/sharenvme/usershome/cyl/test/model/mistralai/Mixtral-8x7B-Instruct-v0.1', 'enforce_eager': True, 'disable_custom_all_reduce': True}
[0;36m(APIServer pid=2159184)[0;0m INFO 01-25 21:13:54 [model.py:526] Resolved architecture: MixtralForCausalLM
[0;36m(APIServer pid=2159184)[0;0m INFO 01-25 21:13:54 [model.py:1523] Using max model len 32768
[0;36m(APIServer pid=2159184)[0;0m WARNING 01-25 21:13:54 [cpu.py:157] VLLM_CPU_KVCACHE_SPACE not set. Using 251.94 GiB for KV cache.
[0;36m(APIServer pid=2159184)[0;0m INFO 01-25 21:13:54 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=2159184)[0;0m INFO 01-25 21:13:54 [vllm.py:635] Disabling NCCL for DP synchronization when using async scheduling.
[0;36m(APIServer pid=2159184)[0;0m INFO 01-25 21:13:54 [vllm.py:640] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2159184)[0;0m WARNING 01-25 21:13:54 [vllm.py:664] Enforce eager set, overriding optimization level to -O0
INFO 01-25 21:13:58 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.
INFO 01-25 21:13:58 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:00 [core.py:96] Initializing a V1 LLM engine (v0.1.dev22+gddd454f3a.d20260125) with config: model='/sharenvme/usershome/cyl/test/model/mistralai/Mixtral-8x7B-Instruct-v0.1', speculative_config=None, tokenizer='/sharenvme/usershome/cyl/test/model/mistralai/Mixtral-8x7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False), seed=0, served_model_name=/sharenvme/usershome/cyl/test/model/mistralai/Mixtral-8x7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:39] testvv
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:180] auto thread-binding list (id, physical core): [(64, 0), (65, 1), (66, 2), (67, 3), (68, 4), (69, 5), (70, 6), (71, 7), (72, 8), (73, 9), (74, 10), (75, 11), (76, 12), (77, 13), (78, 14), (79, 15)]
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] OMP threads binding of Process 2159468:
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159468, core 64
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159680, core 65
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159681, core 66
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159682, core 67
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159683, core 68
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159684, core 69
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159685, core 70
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159686, core 71
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159687, core 72
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159688, core 73
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159689, core 74
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159690, core 75
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159691, core 76
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159692, core 77
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159693, core 78
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 	OMP tid: 2159694, core 79
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [cpu_worker.py:86] 
[0;36m(EngineCore_DP0 pid=2159468)[0;0m INFO 01-25 21:14:02 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.16.33.30:50063 backend=gloo
